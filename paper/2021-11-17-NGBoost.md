---
title: NGBoost, Natural Gradient Boosting for Probabilistic Prediction
toc: true
author_profile: false
categories: ML
tag: NGBoost
use_math: True
---

# NGBoost Review

출처: http://proceedings.mlr.press/v119/duan20a/duan20a.pdf

## Abstract

NGBoost는 확률적 예측을 사용하는 Gradient Boosting 알고리즘이다.  
일반적인 회귀 모델은 공변량을 조건으로 점 추정치를 반환하지만 확률적 회귀 모델은 공변량을 조건으로 결과 공간에 대한 전체 확률 분포를 출력한다. NGBoost는 조건부 분포의 매개변수(Parameter)들을 다중 매개변수 부스팅 알고리즘 (Multiparameter boosting algorithm)으로 처리하여 확률적 회귀에 대한 Gradient Boosting을 일반화한다. 또한, multiparameter boosting approach를 위한 Natural Gradient를 도입하였다. 

## 2. Summary of Contributions

>i. Natural Gradient Boosting 제시  
ii. CRPS와 같은 다른 Scoring rule에 대한 Natural Gradient의 일반화를 제시  
iii. NGBoost 가 점추정치 예측 모델이나 예측 불확실성 추정치 모델들에 비해 좋은 결과를 가진다.

## 3. Natural Gradient Boosting
일반적인 다른 회귀 모델에서는 추정치를 $E[y|x]$ 로 세팅하지만, 이 확률적 회귀 모델에서는 $P_\theta(y|x)$ 의 확률분포를 찾는데에 초점을 둔다. 여기서 $P_\theta(y|x)$는 모수를 가진 형태이며, 매개변수 $\theta$를 가진 함수 p를 $x$의 함수로써 추정하는 것이다. 

### 3.1 Proper Scoring Rules
정의: $S(P,y)$  
예측확률 분포 $P$ 와 하나의 실제 관측치 $y$ 를 입력으로 받아서 결과의 실제 분포가 예상에서 가장 좋은 점수를 받도록 예측에 점수 $S(P,y)$ 를 할당한다.

수학적으로 보면 아래의 (1)식을 만족하면 적절한 Scoring rule이라고 말한다.

$$
\begin{equation}
\begin{aligned}
E_{y \sim Q}[S(Q,y)] \leq E_{y\sim Q}[S(P,y)] 
\end{aligned}
\end{equation}
$$  
where $Q$ : y의 실제 분포, $P$: 모델링에서의 확률적 예측 분포

적절한 Scoring rule을 최소화하는 방법은 두가지가 있다.  
1. MLE
$$
\begin{equation}
\begin{aligned}
L(\theta,y) = -logP_\theta(y)
\end{aligned}
\end{equation}
$$  

2. CRPS
$$
\begin{equation}
\begin{aligned}
C(\theta,y) = \int_{-\infty}^y{F_\theta(z)^2}dz + \int_{-\infty}^y{(1-F_\theta(z))^2}dz
\end{aligned}
\end{equation}
$$ 

### 3.2 The Generalized Natural Gradient
표준 경사 하강법을 생각해보면 Score의 negative gradient 방향으로 이동하면서 점수 규칙을 최소화하는 파라미터들을 찾는다. 그 방향이 가장 가파른 Score의 상승 방향이고 파라미터들을 그 방향으로 무한히 작은 양만큼 이동하는 것이 Scoring rule 을 가장 많이 증가시킨다.

$$
\begin{equation}
\begin{aligned}
\nabla S(\theta,y) \propto \underset{\epsilon \rightarrow 0}{\operatorname{lim}}\,\underset{d:\,||d||=\,\epsilon}{\operatorname{argmax}}\, S(\theta + d,\,y)
\end{aligned}
\end{equation}
$$ 

이 Gradient들은 재매개변수화 하면 변한다. 파라미터 $\theta$ 가 바뀌면 $P_\theta(y \in A)$ 의 값이 달라진다는 소리다. 이 문제는 두 파라미터 값들 사이의 거리가 그 파라미터에 해당하는 분포들 사이의 적절한 거리를 대변해주지 못하기 때문이다. 이것이 정보기하학에서 natural gradient를 사용하도록 하는 동기가 되었다.

**Divergences.**
여기서 Divergneces는 발산을 얘기하는 것이 아니라 두 분포의 Score의 기대값의 차이를 얘기하는 것이다.

$$
\begin{equation}
\begin{aligned}
D_S(Q||P) = E_{y\sim Q}[S(P,y)] - E_{y \sim Q}[S(Q,y)]
\end{aligned}
\end{equation}
$$ 

(1) 의 부등식을 만족하므로 $D_S(Q||P)$ 는 0 이상의 값을 가진다.  
MLE scoring rule 은 Kullback-Leibler divergence $(D_{KL})$, CRPS 은 $L^2$ divergence $(D_{L^2})$ 를 유도한다. 두 divergence 모두 파라미터에 따라 변하지 않는다. 

<br>

**Natural Gradient.**

리만 공간에서 Score가 가장 빠르게 올라가는 방향이 generalized natural gradient 가 된다.

$$
\begin{equation}
\begin{aligned}
\tilde{\nabla} S(\theta,y) \propto \underset{\epsilon \rightarrow 0}{\operatorname{lim}}\,\underset{d:\,D_S(P_\theta||P_{\theta+d})=\,\epsilon}{\operatorname{argmax}}\, S(\theta + d,\,y).
\end{aligned}
\end{equation}
$$ 


(6) 식에 해당하는 최적화문제를 풀면 라그랑주 승수법을 이용하여 풀면 (7) 식과 같은 form의 natural gradient를 얻을 수 있다.

$$
\begin{equation}
\begin{aligned}
\tilde{\nabla} S(\theta,y) \propto I_S(\theta)^{-1} \nabla S(\theta,y)
\end{aligned}
\end{equation}
$$

$I_S(\theta)$ 는 Scoring rule $S$ 에 의해 유도되는 통계 매니폴드의 리만 메트릭(Riemmanian metric) 이다. 원래의 Nature gradient는 $D_{KL}$에 의해 유도되지만, 이 모델에서는 일부 적절한 Scoring rule에 해당하는 어떠한 divergence도 적용하도록 하였다.

<br>

 $S=L$ 로 선택함으로써(MLE 방법 선택) 위의 최적화를 풀면


$$
\begin{equation}
\begin{aligned}
\tilde{\nabla} L(\theta,y) \propto I_L(\theta)^{-1} \nabla L(\theta,y)
\end{aligned}
\end{equation}
$$

$I_L(\theta)$ 는 $P_\theta$ 에 대한 관측치에 의한 피셔정보(Fisher Information)에 해당한다.

$$
\begin{equation}
\begin{aligned}
I_L(\theta) = E_{y\sim P_\theta}\big[\nabla_\theta L(\theta,y)\nabla_\theta L(\theta,y)^T\big]
\end{aligned}
\end{equation}
$$

위와 비슷하게 $S=C$ 로 선택함으로써(CRPS 방법 선택) 위의 최적화를 풀면

$$
\begin{equation}
\begin{aligned}
\tilde{\nabla} C(\theta,y) \propto I_C(\theta)^{-1} \nabla C(\theta,y)
\end{aligned}
\end{equation}
$$

$I_C(\theta)$ 는 $D_{L^2}$를 국소 거리(local distance)로 사용하는 통계 매니폴드(Statistics manifold)의 리만 메트릭이다.


$$
\begin{equation}
\begin{aligned}
I_C(\theta) = 2 \int_{-\infty}^{\infty}\nabla_\theta F_\theta(\theta,y)\,\nabla_\theta F_\theta(\theta,y)^T
\end{aligned}
\end{equation}
$$

파라미터를 학습하기 위해 natural gradient를 사용하면 최적화 문제가 파라미터가 변해도 불변하고, 보다 효율적이고 안정적인 학습을 할 수 있도록 한다.

### 3.4 NGBoost : Natural Gradient Boosting

NGBoost 알고리즘의 수도코드는 아래와 같다.

<img src="/assets/images/ngboost.png" width="90%" height="90%" title="수도코드" alt="1"/>

### 3.5 Analysis and Discussion

**Boosting for Probabilistic Prediction**  

NGBoost의 장점은 사용자가 실제 값 파라미터들에 의해 식별되는 다른 분포군을 지정하는 것이 자유롭고, 이런 모든 파라미터들이 평균뿐만 아니라 공변량(covariate) 전체에 걸쳐 달라질 수 있다는 것이다. 따라서, 다양하게 확장되어 이용될 수 있다.

**Multiparameter Boosting**  

multiparameter를 가지는 광범위한 Gradient Boosting의 확장은 분포 예측 문제를 Scoring rule에 따라 파라미터 당 하나씩 $x$ 의 $P$ 함수를 공동으로 추정하는 문제로 전환함으로써 가능하다. 이 설정에서는 단계 승수(step multiplier)에 대한 전체적인 라인검색(line search) 이 불가피한 결과이다. 그러나 이 방법에서 natural gradient의 사용은 모든 객체가 피셔 정보 인자로 인해 "optimal pre-scaled" 되어 오기 때문에 이러한 문제가 덜해진다. 각 반복에서 스케일링 인자 $p^(m)$에 달라지더라도 natural gradient를 통해 파라미터들은 초기 주변 분포로부터 다른 조건부 평균,분산 및 거리 에도 불구 하고 거의 동일한 속도로 수렴한다. 이러한 안정성을 natural gradient의 "optimal pre-scaling" 속성이라고 한다.

**Parameterization**  

확률분포가 지수족이고 파라미터의 선택이 해당 족의 자연 파라미터인 경우 뉴턴-랩슨 단계가 natural gradient 단계와 동일하다. 그러나 다른 분포에서는 동등성이 유지될 필요가 없다.

**Computational Complexity**

NGBoost와 표준 부스팅 알고리즘과 두 가지 계산 차이가 있다.  
1. NGBoost는 각 파라미터에 적합해야 하는 반면, 표준 부스팅 방법은 하나의 학습자만 적합한다.
2. NGBoost는 관측치당 자연 기울기를 계산하는데 관측치 수만큼 $p\times p$ 차원( $p$ 는 파라미터 수)의 $I_S^{-1}$ 를 계삲여야 한다. 이 계산 비용의 스케일은 $p^3$ 이고 관측치 $N$에 비례한다. 실제로 사용되는 분포는 대부분 모수가 두개 이하이므로 이러한 계산 비용을 크게 걱정하지는 않아도 된다. 
