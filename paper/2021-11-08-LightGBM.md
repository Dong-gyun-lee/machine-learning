---
title: LightGBM, A Highly Efficient Gradient Boosting Decision Tree
toc: true
author_profile: false
categories: ML
tag: LightGBM
use_math: True
---

# LightGBM Review

출처: https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76

## 1. Introduction

기존의 Gradient Boosting machine 에서는 모든 분할점(Split point)에 대해 정보 획득을 평가하기 위해서, 모든 객체를 스캔하다보니 이에 많은 시간이 소요되는 문제점이 있었다. 이는 bucket을 활용하여 구간을 나누어서 Split 과정을 진행하는 XGBoost에서 많이 완화가 되었다. 이에 더해 LightGBM에서는 데이터 객체와 feature의 개수를 더 줄임으로써 계산 과정을 더 줄이고자 하였다. 그래서 다음과 같은 두 가지 방법을 고안하였다.

1. Gradient-based One-Side Sampling(GOSS)
2. Exclusive Feature Bundling(EFB)  

## 2. Preliminaries

<img src="/assets/images/lightgbm1" width="90%" height="90%" title="수식 전개" alt="1"/>

Algorithm 1: Histogram-based Algorithm

>기존의 XGBoost 알고리즘에서 사용한 것과 같다.

Algorithm 2: Gradient-based One-side Sampling

>gradient들을 크기순으로 나열한 후, topN은 gradient가 높은 $(a\times len(I))$ 개의 객체들, randN은 gradient가 낮은 $(b\times len(I))$ 개의 객체들로 지정하고 gradient가 높은 객체들은 무조건 학습에 포함 시키고, 낮은 객체들은 랜덤 확률로 제외를 시킨다. gradient가 가장 낮은 객체들은 차이가 별로 안날 것이므로 학습에 더 포함을 시키지 않아도 크게 모델의 error에 영향을 끼치지도 않을 것이라고 판단되므로 합리적인 생각인 것 같다. 또한 모델이 작은 gradient도 학습하면서 너무 Overfitting 되는 것을 방지하는 효과도 생각한 것 같다.  

## 4. Exclusive Feature Bundling

<img src="/assets/images/lightgbm2" width="90%" height="90%" title="수식 전개" alt="2"/>

Greedy Bundling은 변수들은 어떤 방식으로 묶을지에 해당하는 알고리즘이고, Merge Exclusive Features는 같이 묶인 변수들을 어떻게 합칠지에 해당하는 알고리즘이다.

일단 변수들이 Sparse한 구조를 가진다고 가정한다. Sparse한 구조이므로 변수들의 분포가 비슷한 분포보다 안 비슷한 분포를 찾는 것이 더 쉬울 것이다.

Algorithm 3: Greedy Bundling  

>알고리즘을 간단하게 설명해보자. 범주형인 Sparse한 구조를 가진 변수 두 개를 비교한다고 가정하면, 두 변수 다 0이 아닌 값을 가질 때 conflict 하다고 할 수 있다. 모든 변수를 대상으로 비교를 하여 adjacency matrix를 만든다. 그 다음 cut-off 할 값을 정해 conflict를 어느 정도까지 허용할 지를 정한다. cut-off 값 밑에 해당하는 conflict 하지 않는 변수들을 bundling 한다. 
bundling 은 adjacency matrix의 column sum을 degree로 설정하여 degree가 큰 변수부터 기준 변수로 하여 순서대로 진행해나간다. graph 구조로 adjacency matrix를 나타내면 bundling 결과를 쉽게 예측해볼 수 있다. 

Algorithm 4: Merge Exclusive Features

>변수들을 merge 하는 방법은 기준 변수가 0 값이고, 합쳐질 변수가 양수 값이면, **기준 변수의 max값 + 합쳐질 변수의 값** 을 기준변수에 넣어준다. 반대의 경우이거나, conflict한(둘 다 양수) 객체일 경우 기준 변수의 값을 채택한다.  
conflict 하지 않는 경우 기준변수의 값은 max 값 밑의 분포, 합쳐질 변수의 값을 max 값 위의 분포를 가짐으로써 두 변수의 정보를 거의 잃지 않는다는 것을 알 수 있다. conflict한 객체의 경우 합쳐질 변수의 정보를 버리기는 하지만 cut-off를 통해 최소한의 정보만을 버리도록 하고 있다. 변수 여러 개가 합쳐질 때도 같은 방식으로 계속 진행하면 된다.

## Comments

Sparse한 구조를 가지는 feature들을 bundling 해줌으로써 계산과정을 더 빠르고 간단하게 하는 방법을 깨달을 수 있었다.

LightGBM은 기존의 CART 알고리즘과 다르게 leaf-wise 방식을 사용한다고 한다. leaf-wise는 기존의 level-wise 방식과 다르게 대칭적은 tree를 만들어주지 않아도 되서 계산이 더 빨라지고 정확도도 좋아지는 장점이 있다고 한다. 하지만, 단점이 무엇이 있길래 다른 알고리즘에서는 사용하지 않았는지는 잘 모르겠다..