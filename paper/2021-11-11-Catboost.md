---
title: CatBoost, unbiased boosting with categorical features
toc: true
author_profile: false
categories: ML
tag: CatBoost
use_math: True
---

# CatBoost Review

출처: https://arxiv.org/pdf/1706.09516.pdf

## 1. Introduction
  
Catboost는 기존의 Gradient boosting 방법들에 존재하는 target leakage로 인해 발생되는 prediction shift 문제를 보완하기 위해 만들어진 방법이다.


## 2. Background

Background에서는 기존의 Gradient Boosting에 대한 설명을 하고 있으므로 자세한 설명은 생략한다.

## 3. Categorical features
### 3.1 Related work on categorical features  
  
범주형 변수(Categorical features)들은 서로 비교할 수 없는 카테고리를 가진 이산형의 집합이다. 보통 이러한 변수들은 One-hot encoding 들을 하여 변수들을 처리해주는데, 범주가 너무 많을 경우에 One-hot encoding을 할 시에 변수가 너무 많아지는 현상이 발생한다. 이를 보완해주기 위한 방법으로 Target Statistics(TS) 를 추정하여 각 범주에 target value 값을 지정해주는 방법을 많이 사용한다.

### 3.2 Target statistics

일반적으로 TS 값은 아래와 같이 구한다.
$\hat{x}_k^i \approx E[y|x^i=x_k^i]$  

$i$: Categorical feature   
$k$: training example index  

<br>

**Greedy TS**  

$\hat{x}_k^i = \frac{\sum_{j=1}^n{\mathbb{1}_{\left\{x_j^i=x_k^j\right\}}\cdot\, y_j \,+\, ap}}{\sum_{j=1}^n\mathbb{1}_{\left\{x_j^i=x_k^j\right\}}\,+\,a}$ , where $a>0$ is a parameter.

$\hat{x}_k^i = \frac{\sum_{j=1}^n1_{\{x_j^i=x_k^j\}}\cdot\, y_j \,+\, ap}{\sum_{j=1}^n1_{\{x_j^i=x_k^j\}}\,+\,a}$

여기서 $p$는 target value 의 평균으로 보통 설정한다.  

위의 값을 계산 시 target leakage가 발생하여 conditional shift를 야기시킬 수 있다. 

- target leakage: $\hat{x}_k^i$ 를 계산하는 데 target 값인 $y_k$가 사용되는 문제
- conditional shift: 학습 데이터에서는 실제 target value 값을 알고, 그 값을 사용하여 TS 값을 구하므로 학습 데이터에 해당하는 조건부 분포 $\hat{x}^i|y$ 의 분포와 테스트 데이터에 해당하는 $\hat{x}^i|y$ 의 분포가 다르다.


위의 문제들 때문에 자기 자신 객체의 target value를 사용하지 않고 TS를 추정하는 방법을 통해 이를 보완하고자 하였다.  

**Holdout TS**  

학습 데이터를 2개 파트로 분할 $D = \hat{D_0}\cup\hat{D_1}$  
> $\hat{D_0}$ : TS 계산 시에 사용  
$\hat{D_1}$ : 모델 학습에 사용
- TS 계산 시에 모든 데이터를 이용하는 것이 아니라 일정비율만 사용하므로 주어진 데이터를 최대한 활용할 수 없다는 단점이 있다.  

**Leave-one-out TS**  

$D_k = D\backslash x_k$ for training examples : 자기 자신 객체 $x_k$만을 뺀 것을 $D_k$ 로하여 통계량을 계산한다.  
$D_k = D$ for test ones
- 이 방법은 target leakage를 막는데는 도움이 되지 못한다. 학습데이터에서는 target leakage를 막아주어 변별력이 있는 것처럼 보이지만 테스트 데이터에서는 TS가 모두 동일한 값을 가져서 Split 하지 못하는 문제가 발생할 수 있다.

**Ordered TS**  

객체들을 artificial time을 적용하여 랜덤하게 permutaion 시킨다. 학습 데이터들의 random permutaion을 $\sigma$ 라고 하자.
$D_k = \left\{x_k:\sigma (j)<\sigma (k)\right\}$ for training examples  
$D_k = D$ for test ones  
- 자기 자신보다 전 시점의 있는 데이터 객체들로만 TS값을 계산한다.
- 가장 오래 전에 만들어진 객체들의 최근의 객체들보다 여러 번 사용될 것이므로 random permutaion을 여러 번 수행하도록 한다.
- Catboost에는 이 Ordered TS 방법을 이용하여 target leakage 문제를 해결하려고 노력하였다.

## 4. Prediction shift and ordered boosting
### 4.1 Prediction shift
$h_t = \underset{h \,\in \,H}{\operatorname{argmin}}\, E(-g^t(x,y)-h(x))^2 \approx \underset{h \,\in \,H}{\operatorname{argmin}}\,\frac{1}{n}\sum_{k=1}^n(-g^t(x,y)-h(x))^2$  
- $h_t$를 구할 때, 기대값이 알려진 바 없으므로 dataset $D$ 로 근사치를 구하는 과정에서 학습데이터의 조건부 분포가 테스트 데이터의 조건부 분포와 다른 문제가 있다.  
정확히 이야기 해보자면 $F^t$ 를 학습시에 $-g^t(x_k,y_k)$ 에서 $y_k$(자기 자신의 반응 변수)를 사용함으로써 target leakage가 발생하고, 테스트 데이터에서는 자기 자신의 반응변수가 영향을 끼칠 일이 없으므로, 이로 인해 학습 데이터에서 Prediction shift가 일어나는 것이다.  
이에 대한 해결방안으로 간단하게 생각해보면 잔차 $-g^t(x_k,y_k)$를 구하는 과정에서 자기 자신의 객체를 빼고 구하는 방법을 생각해볼 수 있다.


### 4.2 Ordered Boosting

Ordered TS를 구할 때와 비슷한 방식을 사용하여 모델링을 진행한다. 객체들을 artificial time을 적용하여 랜덤하게 permutaion 시킨다. 학습 데이터들을 대상으로 random permutaion을 진행하여 $\sigma$의 순서를 부여하고 $M_1$ 은 첫번째 데이터, $M_2는 처음 두개의 데이터를 가지고 모델링 학습을 하고,  잔차를 구한다. i번째 모델은 같은 방식으로 i개의 데이터를 가지고 모델링 학습을 한다.  

$M_i, i=1,\cdots n$ 

모델링 학습을 하고, 그 모델로 다음 시점의 데이터의 잔차를 구한다.

$-g^t(x_k,y_k)=r^t(x_k,y_k)\,=\,y-M_{t-1}(x_k)$

## 5. Practical implementation of ordered boosting

ordered boosting 방법의 알고리즘을 수도코드로 나타내면 아래와 같다.

<img src="/assets/images/catboost1.png" width="90%" height="90%" title="수식 전개" alt="1"/>


Catboost에 사용되는 알고리즘을 수도코드로 나타내면 아래와 같다.

<img src="/assets/images/catboost2.png" width="90%" height="90%" title="수식 전개" alt="2"/>

## 6. Comment

기존의 Gradient Boosting 알고리즘에서 target leakage의 문제를 해결하기 위해 Ordered TS, Ordered Boosting 방법을 첨가하여 만들었다는 것이 기존의 알고리즘과 가장 큰 차이점이라고 생각한다.  
원래 Gradient boosting 류의 알고리즘을 코딩으로 돌려볼 때, 범주형 변수들의 처리는 One-hot encoding만 고려해 보았었는데 이 논문을 읽고 나서 target leakage 문제를 해결하는 target statistics 도 고려해보아야한다는 것을 깨달았다.