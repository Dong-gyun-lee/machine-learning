---
title: XGBoost, A Scalable Tree Boosting System
toc: true
author_profile: false
use_math: True
---

# XGBoost Review

출처:https://dl.acm.org/doi/10.1145/2939672.2939785

## 1. Introduction

XGBoost는 다음과 같은 기여를 하는 것을 목표로 한다. 

- We design and build a highly scalable end-to-end tree
boosting system.
- We propose a theoretically justified weighted quantile
sketch for efficient proposal calculation
- We introduce a novel sparsity-aware algorithm for parallel tree learning
- We propose an e↵ective cache-aware block structure
for out-of-core tree learning.

<br>

## 2. TREE BOOSTING IN A NUTSHELL
### 2.1 Regularized Learning Objective

$$
\begin{equation}
\begin{aligned}
\hat{y_i} = \phi(x_i) = \sum_{k=1}^{K}f_k(x_i), \quad f_k \in F,
\end{aligned}
\end{equation}
$$

where $F = {f\{x = w_{q(x)}\}}(q\,:\,R^m \rightarrow T , \,w \in R^T)$

- objective function을 위의 additiv function으로 정의할 수 있다. $F$ 는 CART 라고 알려진 regression tree의 space를 정의한다.

$$
\begin{equation}
\begin{aligned}
L(\phi) & = \sum_{i}l(\hat{y_i},y_i) + \sum_k{\Omega(f_k)}\\
where & \,\, \Omega(f)=\gamma T+\frac{1}{2}\lambda \Vert {w}\Vert^2
\end{aligned}
\end{equation}
$$

- 여기서 $l$ 은 미분 가능한 loss function 이다. XGboost에서는 뒤의 페널티 항을 통하여 마지막 학습 값을 Smooth 하게 하고, over-fitting 을 피할 수 있도록 한다.

## 2.2 Gradient Tree Boosting

<br>


$
\begin{aligned}
L^{(t)} = obj^(t) = \sum_{i=1}^nl(y_i,\hat{y_i}^{(t-1)}+f_t(x_i)) + \Omega(f_t)
\end{aligned}
$

<br>

- equation(2)에서는 매개변수로 함수를 포함하므로 최적화를 시키기 어렵다. 그러므로 t번째 반복에서의 loss function을 아래와 같이 고려하여 최적화를 진행한다. <br>
여기서 $f_t$ 는 모델을 향상시키는 방향으로 추가하는 항목이다. 


$
\begin{aligned}
L^{(t)} \approx obj^(t) = \sum_{i=1}^nl(y_i,\hat{y_i}^{(t-1)}+f_t(x_i)) + \Omega(f_t)
\end{aligned}
$

where $g_i = \partial_{\hat{y}^{(t-1)}}l(y_i,\hat{y_i})$, $g_i = \partial^2_{\hat{y}^{(t-1)}}l(y_i,\hat{y_i})$

- 2차 테일러 전개를 통해 더 빠르게 최적화를 할 수 있는 세팅을 위와 같이 만들어 준 것 이다.

$$
\begin{equation}
\begin{aligned}
\tilde{L}^{(t)} & = \sum_{i=1}^n[g_if_t(x_i))+h_if_t^2(x_i)) ]+ \Omega(f_t)
\end{aligned}
\end{equation}
$$

- 위의 $L^{(t)}$ 식의 Constant에 해당하는 부분을 제거하고 (3)번 식과 같이 간편화한다.

$$
\begin{equation}
\begin{aligned}
\tilde{L}^{(t)} & = \sum_{i=1}^n[g_if_t(x_i))+h_if_t^2(x_i)) ]+ \gamma T + \frac{1}{2}\sum_{i=1}^{T}w_j^2 \\
\, & = \sum_{j=1}^T[(\sum_{i\in I_j}g_i)w_j+\frac{1}{2}(\sum_{i\in I_j}h_i+\lambda)w_j^2]+\gamma T
\end{aligned}
\end{equation}
$$

- 계속 전개해보면 위의 (4)번식과 같은 꼴을 나타낼 수 있다.

$$
\begin{equation}
\begin{aligned}
w_j^*=-\frac{\sum_{i\in I_j}g_i}{\sum_{i\in I_j}h_i+\lambda}
\end{aligned}
\end{equation}
$$

-(4)번 식에서 loss function을 가장 작게 만들어주는 근을 구하면 (5)번 식처럼 나온다.

$$
\begin{equation}
\begin{aligned}
\tilde{L}^{(t)}(q)=-\frac{1}{2}\sum_{j=1}^{T}\frac{(\sum_{i\in I_j}g_i)^2}{\sum_{i\in I_j}h_i+\lambda}+\gamma T.
\end{aligned}
\end{equation}
$$

- (5)의 근을 (4) 식에 넣으면 (6) 식처럼 전개가 된다.
- 2.2의 전개 과정을 아래에 수식으로 전개한 것을 이미지로 첨부하여본다.

<br>

이미지

<br>

$$
\begin{equation}
\begin{aligned}
L_{split}=\frac{1}{2}\Bigg[\frac{(\sum_{i\in I_L}g_i)^2}{\sum_{i\in I_L}h_i+\lambda}+\frac{(\sum_{i\in I_R}g_i)^2}{\sum_{i\in I_R}h_i+\lambda}-\frac{(\sum_{i\in I}g_i)^2}{\sum_{i\in I}h_i+\lambda}\Bigg]-\gamma
\end{aligned}
\end{equation}
$$

- 위의 식들을 통해서 loss reductin을 $L_{split}$ 과 같이 정의할 수 있다.
- pre-stopping 방법은 gain(= $L_{split}$ )이 negative 이면 split을 멈추는 방법이다. <br>
post-prunning 방법은 negative gain 부분을 split을 가지치기 하는 방법이다. <br>
결론적으로 $L_{split}$ 값이 양수(positive) 값이어야 그 노드(잎)에서 split이 계속 진행되는 모델이 된다.

## 2.3 Shrinkage and Column Subsampling

<br>

두 방법 다 overfitting을 예방하기 위한 효과를 가지고 있다. <br>
Shrinkage는 GBM 에서도 사용했던 learning rate의 개념이라고 생각하면 된다. <br> 
Column subsampling은 randomForest에서도 사용하는 방법과 비슷한데, column feature를 전체 다 사용하지 않고 몇 개의 column만 골라서 사용하는 방법이다. subsampling 의 사용은 컴퓨터 병렬 알고리즘의 속도도 가속화 시켜준다고 한다.


<br>

## 3.  SPLIT FINDING ALGORITHMS
### 3.1 Basic Exact Greedy Algorithm

**Algorithm** :  
> $gain \leftarrow 0$  
$G \leftarrow \sum_{i\in I}g_i, \,\, H \leftarrow \sum_{i\in I}h_i$  
**for** k=1 **to** m **do**:    
$\quad G_L \leftarrow 0, H_L \rightarrow 0$  
$\quad$ **for** $j$ in sorted($I,\,by\,x_{jk} $) **do**:  
$\qquad\quad  G_L \leftarrow G_L+g_j,\, H_L \leftarrow H_L + h_j$  
$\qquad\quad  G_R \leftarrow G+G_L,\, H_R \leftarrow H - H_L$  
$\qquad\quad  score \leftarrow max(score, \frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{G^2}{H+\lambda})$  
$\quad$ **end**  
**end**  
**Output**: Split with max score

- 장점: 항상 Optimal Split을 찾아줄 수 있다.
- 단점: 데이터가 너무 크면 메모리에 다 들어가지 못해서 병렬처리가 불가능할 수 있다.

<br>

### 3.2 Apporximate Algorithm for Split Finding

<br>

**Algorithm**:
> **for** k=1 **to** m **do**  
$\quad$ Proposed $S_k=\{s_{k1},s_{k2},\cdots,s_{kl}\}$ by percentiles on feature $k$ .   
$\quad$ Proposal can be done per tree (global), or per split(local).  
**end**
> **for** k=1 **to** m **do**  
$\quad G_{kv}\leftarrow=\sum_{j\in\{j|s_{k,v}\geq x_{jk} > j|s_{k,v-1} \}}g_j$   
$\quad H_{kv}\leftarrow=\sum_{j\in\{j|s_{k,v}\geq x_{jk} > j|s_{k,v-1} \}}h_j$  
**end**  
Follow same step as in previous section to find max  
score only among proposed splits.

알고리즘은 간단하게 설명해보면 처음에 데이터들을 $l$개의 구간으로 나눈다. 그 하나하나의 구간을 버킷이라고 하는데, 각 버킷에서의 gradient를 계산하여 최적의 Split을 찾아서 나누게 된다.  
Global variant는 처음에 $l$개의 버킷으로 나눈 것을 split해도 총 버킷의 개수를 $l$개를 최대한 유지하도록 하는 방법이고,  
Local variant는 $l$개의 버킷으로 나눈 것을 split하고 나온 left child, right child 들도 $l$개의 버킷을 가지도록 하는 것이다. 

<br>

### 3.3 Weighted Quantile Sketch

<br>

XGBoost는 분포를 이용하는 방법에서 Weighted Quantile을 이용해서 분기지점을 찾는다.

<br>

### 3.4 Sparsity-aware Split Finding

<br>

데이터에 결측치가 있거나, Sparse 한 데이터 혹은 one-hot encoding을 통해 Sparse하게 만들어 준 데이터를 CART 방법으로 Split 하기에 어려움이 있어서 고안해낸 솔루션이다.  
각 트리 노드의 기본 방향을 왼쪽 혹은 오른 쪽으로 미리 입력값을 주고, 결측값이 존재할 때 default 값으로 분류하여 모델링을 학습하는 것이다. 두 방향을 모두 실험해보고 Score(= Gain)이 큰 경우를 선택하여 모델을 학습해 나간다.

<br>

## 4. SYSTEM DESIGN

아래의 3가지 방법을 통해서 GBM에서 문제였던 메모리와 속도 문제의 해결방법을 제시하였다.
자세한 방법은 CS쪽 전공자가 아니어서 잘 모르겠다.

- Column Block for Parallel Learning
- Cache-aware Access
- Blocks for Out-of-core Computation


## Comments

Gradient Boosting machine 에서 좀 더 확장성을 가진 모델이라고 한다.  
Exact Greedy Algorithm 에서 메모리 문제가 발생한 것에 대한 해결 방안으로 Approximate Algorithm을 제시한 것이 인상 깊은 방법이었다. 통계적인 부분에서 GBM 과의 또 다른 차이점은 일반 tree 모델의 방법을 가져왔음에도 Column subssampling을 해줄 수 있다는 점인 것 같다. 또한, 결측치 처리를 데이터 전처리 과정에서 고민해보는 것이 당연히 먼저겠지만 결측치가 있는 데이터를 모델링하여도 미리 방향성을 제시함으로써 모델링을 진행하도록 하는 방법이 인상 깊었다.

XGBOOST는 GBM의 Memory나 속도적인 문제를 통계적인 알고리즘적으로도 해결하려고 하고 System design 쪽으로도 해결하려고 한 방법인데, System design 쪽은 잘 알지 못해서 아쉬웠던 것 같다.